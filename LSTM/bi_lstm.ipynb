{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import collections\n",
    "import re\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "import torch\n",
    "import torchtext\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils import load_dataset, train_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"../dataset/sqli1.csv\")\n",
    "dataset_size = len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(query):\n",
    "    regex = r\"(\\/\\*\\*\\/|\\*\\/|\\/\\*|\\|\\||\\-\\-\\+|\\-\\-|\\&\\&|\\!\\=|\\<\\>|\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}|[\\w]+|.)\"\n",
    "    words = [word for word in re.split(regex, query) if word]\n",
    "    return words\n",
    "\n",
    "def create_vocab(dataset):\n",
    "    counter = collections.Counter()\n",
    "    for query, _ in dataset:\n",
    "        counter.update(tokenizer(query))\n",
    "    return torchtext.vocab.vocab(counter, min_freq=1)\n",
    "\n",
    "vocab = create_vocab(dataset)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "def encode(x):\n",
    "    encoded = []\n",
    "    for str in tokenizer(x):\n",
    "        encoded.append(vocab.get_stoi()[str])\n",
    "    return encoded\n",
    "\n",
    "def to_bow(query):\n",
    "    bow = torch.zeros(vocab_size, dtype=torch.float32)\n",
    "    for word_id in encode(query):\n",
    "        bow[word_id] += 1\n",
    "    return bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(dataset):\n",
    "    df = torch.zeros(vocab_size)\n",
    "    for query, _ in dataset:\n",
    "        for word_id in set(encode(query)):\n",
    "            df[word_id] += 1\n",
    "    return df\n",
    "\n",
    "def create_tf_idf(input):\n",
    "    df = create_df(dataset)\n",
    "    bow = to_bow(input)\n",
    "    return bow * torch.log((dataset_size+1)/(df+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 9.3175, 0.1116,  ..., 0.0000, 0.0000, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "tf_idf = create_tf_idf(\"1' or '1'='1\")\n",
    "print(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "def process_batch(batch):\n",
    "    querys = torch.Tensor()\n",
    "    labels = []\n",
    "    for query, label in batch:\n",
    "        querys = torch.cat((querys, create_tf_idf(query)))\n",
    "        labels.append(int(label))\n",
    "\n",
    "    return (querys,\n",
    "            torch.LongTensor(labels))\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, collate_fn=process_batch, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BiLSTMNN, self).__init__()\n",
    "        self.f_lstm = torch.nn.LSTM(10906, 32, batch_first=True)\n",
    "        self.b_lstm = torch.nn.LSTM(10906, 32, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(64, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.f_lstm(x)\n",
    "        x = self.b_lstm(torch.flip(x))\n",
    "        x = torch.add(x[0], x[1])\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "network = BiLSTMNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traing...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "LSTM: Expected input to be 2D or 3D, got 1D instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/yukikase/projects/SQLi_detection/BiLSTM/bi_lstm.ipynb Cell 9\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yukikase/projects/SQLi_detection/BiLSTM/bi_lstm.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(network\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mlearning_rate)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yukikase/projects/SQLi_detection/BiLSTM/bi_lstm.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m epoch \u001b[39m=\u001b[39m \u001b[39m50\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/yukikase/projects/SQLi_detection/BiLSTM/bi_lstm.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m train_epoch(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yukikase/projects/SQLi_detection/BiLSTM/bi_lstm.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     network,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yukikase/projects/SQLi_detection/BiLSTM/bi_lstm.ipynb#X15sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     train_loader,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yukikase/projects/SQLi_detection/BiLSTM/bi_lstm.ipynb#X15sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     learning_rate,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yukikase/projects/SQLi_detection/BiLSTM/bi_lstm.ipynb#X15sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     optimizer,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yukikase/projects/SQLi_detection/BiLSTM/bi_lstm.ipynb#X15sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     loss_fn,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yukikase/projects/SQLi_detection/BiLSTM/bi_lstm.ipynb#X15sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     epoch,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yukikase/projects/SQLi_detection/BiLSTM/bi_lstm.ipynb#X15sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     device,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yukikase/projects/SQLi_detection/BiLSTM/bi_lstm.ipynb#X15sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     dataset_size)\n",
      "File \u001b[0;32m~/projects/SQLi_detection/BiLSTM/../utils.py:31\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(network, dataloader, learning_rate, optimizer, loss_fn, epoch, device, dataset_size, report_freq)\u001b[0m\n\u001b[1;32m     29\u001b[0m learning_rate \u001b[39m=\u001b[39m learning_rate\n\u001b[1;32m     30\u001b[0m optimizer \u001b[39m=\u001b[39m optimizer \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(network\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mlearning_rate)\n\u001b[0;32m---> 31\u001b[0m loss_fn \u001b[39m=\u001b[39m loss_fn \u001b[39mor\u001b[39;00m nn\u001b[39m.\u001b[39mNLLLoss()\n\u001b[1;32m     33\u001b[0m total_loss, accuracy, count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m\n\u001b[1;32m     34\u001b[0m epoch_count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/projects/SQLi_detection/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/projects/SQLi_detection/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/Users/yukikase/projects/SQLi_detection/BiLSTM/bi_lstm.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yukikase/projects/SQLi_detection/BiLSTM/bi_lstm.ipynb#X15sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/yukikase/projects/SQLi_detection/BiLSTM/bi_lstm.ipynb#X15sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mf_lstm(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yukikase/projects/SQLi_detection/BiLSTM/bi_lstm.ipynb#X15sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb_lstm(torch\u001b[39m.\u001b[39mflip(x))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yukikase/projects/SQLi_detection/BiLSTM/bi_lstm.ipynb#X15sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((x[\u001b[39m0\u001b[39m], x[\u001b[39m1\u001b[39m]), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/projects/SQLi_detection/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/projects/SQLi_detection/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/SQLi_detection/venv/lib/python3.11/site-packages/torch/nn/modules/rnn.py:845\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    844\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim() \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m):\n\u001b[0;32m--> 845\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLSTM: Expected input to be 2D or 3D, got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim()\u001b[39m}\u001b[39;00m\u001b[39mD instead\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    846\u001b[0m     is_batched \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m\n\u001b[1;32m    847\u001b[0m     batch_dim \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first \u001b[39melse\u001b[39;00m \u001b[39m1\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: LSTM: Expected input to be 2D or 3D, got 1D instead"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(network.parameters(), lr=learning_rate)\n",
    "epoch = 50\n",
    "train_epoch(\n",
    "    network,\n",
    "    train_loader,\n",
    "    learning_rate,\n",
    "    optimizer,\n",
    "    loss_fn,\n",
    "    epoch,\n",
    "    device,\n",
    "    dataset_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
